{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "import json\n",
    "import pathlib\n",
    "from pyspark.sql.functions import from_json, col, row_number, date_from_unix_date\n",
    "from pyspark.sql import Window\n",
    "from delta import DeltaTable\n",
    "\n",
    "\n",
    "class SimpleParser():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.spark =  (\n",
    "            SparkSession\n",
    "            .builder\n",
    "            .master(\"local[*]\")\n",
    "            .appName('spark_demo')\n",
    "            .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,io.delta:delta-spark_2.12:3.1.0')\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "            .config(\"spark.sql.catalog.spark_catalog\",\"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "            .config(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"1\")\n",
    "            .config(\"spark.databricks.delta.snapshotPartitions\", \"2\")\n",
    "            .config(\"spark.ui.showConsoleProgress\", \"false\")\n",
    "            .config(\"spark.ui.enabled\", \"false\")\n",
    "            .config(\"spark.ui.dagGraph.retainedRootRDDs\", \"1\")\n",
    "            .config(\"spark.ui.retainedJobs\", \"1\")\n",
    "            .config(\"spark.ui.retainedStages\", \"1\")\n",
    "            .config(\"spark.ui.retainedTasks\", \"1\")\n",
    "            .config(\"spark.sql.ui.retainedExecutions\", \"1\")\n",
    "            .config(\"spark.worker.ui.retainedExecutors\", \"1\")\n",
    "            .config(\"spark.worker.ui.retainedDrivers\", \"1\")\n",
    "            .config(\"spark.driver.memory\", \"4g\")\n",
    "            .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "            .config(\"spark.driver.extraJavaOptions\", \"-Ddelta.log.cacheSize=3\")\n",
    "            .config(\n",
    "                \"spark.driver.extraJavaOptions\",\n",
    "                \"-XX:+CMSClassUnloadingEnabled -XX:+UseCompressedOops\",\n",
    "            )\n",
    "            .getOrCreate()\n",
    "        )\n",
    "        self.spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "        \n",
    "        self.table_path = \"./spark_data_consumer/data/cdctable\"\n",
    "        self.checkpoint_path = './spark_data_consumer/checkpoints/cdctable'\n",
    "        \n",
    "    def get_schema(self):\n",
    "        schema_path = pathlib.Path(\"spark_data_consumer/app/schema/cdctable.json\").read_text()\n",
    "        schema = StructType.fromJson(json.loads(schema_path))        \n",
    "        return schema\n",
    "    \n",
    "    def read_data(self): \n",
    "        schema = self.get_schema()      \n",
    "        \n",
    "        _df = (\n",
    "            self.spark\n",
    "            .read\n",
    "            .format(\"kafka\")\n",
    "            .option(\"kafka.bootstrap.servers\", \"kafka1:9092\")\n",
    "            .option(\"subscribe\", \"debezium.public.cdctable\")\n",
    "            .load()\n",
    "            .select(\n",
    "                from_json(col(\"value\").cast(\"string\"), schema).alias(\"parsed_value\")              \n",
    "            )\n",
    "            .select(\"parsed_value.*\")\n",
    "        )\n",
    "\n",
    "        return _df\n",
    "    \n",
    "    def read_streaming_data(self): \n",
    "        \n",
    "        schema = self.get_schema()      \n",
    "        \n",
    "        _df = (\n",
    "            self.spark\n",
    "            .readStream\n",
    "            .format(\"kafka\")\n",
    "            .option(\"kafka.bootstrap.servers\", \"kafka1:9092\")\n",
    "            .option(\"subscribe\", \"debezium.public.cdctable\")\n",
    "            .option(\"startingOffsets\", \"earliest\")\n",
    "            .option(\"auto.offset.reset\", \"earliest\")\n",
    "            .load()\n",
    "            .select(\n",
    "                from_json(col(\"value\").cast(\"string\"), schema).alias(\"parsed_value\")              \n",
    "            )\n",
    "            .select(\"parsed_value.*\")\n",
    "        )\n",
    "\n",
    "        return _df\n",
    "    \n",
    "    def _processing_deduplicate(self, df):\n",
    "\n",
    "        df_upsert = df.select(\"after.*\", \"source.txId\").where(\"op in ('c', 'u')\")\n",
    "        df_delete = df.select(\"before.*\", \"source.txId\").where(\"op in ('d')\")\n",
    "\n",
    "        df_combined = df_upsert.unionAll(df_delete)\n",
    "\n",
    "        window_specs = Window.partitionBy('id').orderBy(col('txId').desc())\n",
    "\n",
    "        df_deduplicated = df_combined.withColumn(\n",
    "            \"rn\",\n",
    "            row_number().over(window_specs)\n",
    "        ).where(\"rn = 1\").drop(\"rn\", \"txId\")\n",
    "        \n",
    "        return df_deduplicated\n",
    "    \n",
    "    def _processing_upsert(self, df):\n",
    "        table_exists = DeltaTable.isDeltaTable(sp.spark, self.table_path)\n",
    "\n",
    "        if table_exists:\n",
    "            dt = DeltaTable.forPath(sp.spark, self.table_path)            \n",
    "            (\n",
    "                dt.alias(\"t\")\n",
    "                .merge(df.alias(\"s\"), \"s.id = t.id\")\n",
    "                .whenMatchedUpdateAll()\n",
    "                .whenNotMatchedInsertAll()\n",
    "                # fixme: add deletion case\n",
    "                .execute()\n",
    "            )            \n",
    "        else:\n",
    "            df.repartition(1).write.format(\"delta\").save(self.table_path)\n",
    "            \n",
    "        df.show(truncate=False)\n",
    "                \n",
    "    def _processing_transform(self, df):\n",
    "        return df.withColumn(\"date\", date_from_unix_date(\"date\"))\n",
    "    \n",
    "    def foreach_batch_function(self, df, epoch_id):\n",
    "        # Transform and write batchDF\n",
    "        df.cache()\n",
    "        print(\"Rows to process:\", df.count())\n",
    "        \n",
    "        _d = self._processing_deduplicate(df)  \n",
    "        _t = self._processing_transform(_d)      \n",
    "        self._processing_upsert(_t)\n",
    "        \n",
    "        df.unpersist()\n",
    "    \n",
    "    def start_streaming(self):\n",
    "        \n",
    "        \n",
    "        readStream = self.read_streaming_data()\n",
    "        \n",
    "        command = (\n",
    "            readStream\n",
    "            .writeStream\n",
    "            .option(\"checkpointLocation\", self.checkpoint_path)\n",
    "            .foreachBatch(self.foreach_batch_function)\n",
    "            .start()            \n",
    "        )        \n",
    "        \n",
    "        command.awaitTermination()       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/02/29 16:21:37 WARN Utils: Your hostname, LT479333 resolves to a loopback address: 127.0.1.1; using 192.168.68.60 instead (on interface eth1)\n",
      "24/02/29 16:21:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/avolok/repos/temp/k/.venv/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/avolok/.ivy2/cache\n",
      "The jars for the packages stored in: /home/avolok/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-7df20099-9165-4f14-b2d4-2cdd50423a0e;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound io.delta#delta-spark_2.12;3.1.0 in central\n",
      "\tfound io.delta#delta-storage;3.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 344ms :: artifacts dl 9ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.1.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   14  |   0   |   0   |   0   ||   14  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-7df20099-9165-4f14-b2d4-2cdd50423a0e\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 14 already retrieved (0kB/7ms)\n",
      "24/02/29 16:21:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "sp = SimpleParser()\n",
    "#sp.start_streaming()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sp = SimpleParser()\n",
    "df = sp.read_data()\n",
    "dedup = sp._processing_deduplicate(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+----------+\n",
      "|  id|  name|      date|\n",
      "+----+------+----------+\n",
      "| 226|NHWU3A|2022-10-31|\n",
      "|1241|USY0B9|2019-10-29|\n",
      "|1776|R27V31|2020-02-07|\n",
      "|1892|9LBLP4|2019-03-06|\n",
      "|1977|R11Q2P|2018-09-10|\n",
      "|2258|LBMJQ7|2019-05-03|\n",
      "|2381|GR0ZYX|2022-04-05|\n",
      "|4009|LXU6A7|2022-03-11|\n",
      "|4202|L2U3BA|2020-03-04|\n",
      "|5264|XHJRD3|2023-06-10|\n",
      "|5546|3GZH3S|2020-10-20|\n",
      "|6164|SJWFJW|2021-08-29|\n",
      "|6312|5TAPY2|2021-10-15|\n",
      "|6481|2GQPGR|2023-03-06|\n",
      "|7425|ICEFMO|2021-01-02|\n",
      "|7442|Q1FHQ8|2019-12-03|\n",
      "|7506|WMR1IH|2021-10-13|\n",
      "|7630|R4Z4CX|2023-12-06|\n",
      "|7794|BM4ZZX|2021-06-07|\n",
      "|8077|FG64OV|2020-08-30|\n",
      "+----+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dedup.withColumn(\"date\", date_from_unix_date(\"date\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----+\n",
      "|  id|  name| date|\n",
      "+----+------+-----+\n",
      "|2258|LBMJQ7|18019|\n",
      "|5264|XHJRD3|19518|\n",
      "|7425|ICEFMO|18629|\n",
      "|7442|Q1FHQ8|18233|\n",
      "|7794|BM4ZZX|18785|\n",
      "|9488|8O6BKK|17814|\n",
      "+----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dedup.select(\"*\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
